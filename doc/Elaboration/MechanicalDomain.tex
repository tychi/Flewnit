

\label{sec:mechanicalDomain}
In dieser Domäne wurde exemplarisch eine partikelbasierte Fluid-Simulation nach dem
Verfahren der Smoothed Particle Hydrodynamics mit OpenCL implementiert.
	
\subsubsection{Fluid-Simulation}
		
	\paragraph{Grundlagen}	
		\subparagraph{Die Navier-Stokes-Gleichungen}
		Die Sammlung an Gleichungen, die das Verhalten von Fluiden beschreiben, sind die
		\emph{Navier-Stokes-Gleichungen}.
		Für die Echtzeit-Simulation nützlich sind die \emph{inkompressiblen} Navier-Stokes-Gleichungen:
		\begin{subequations}\label{equ:navStokes1}
			\begin{align}
			\rho 
			\left( 
				\frac{\partial \vec{v}}{\partial t} 
				+ 
				\left( \vec{v} \cdot \nabla	\right) \vec{v} 
			\right)
				& = - \nabla p  + \mu \Delta \vec{v} + \vec{f} \\
			\nabla \cdot \vec{v}   & = 0
			\end{align}	
		\end{subequations}
		
		Die erste Gleichung heißt \emph{Impulsgleichung}. Die basiert auf dem zweiten Newtonschen Gesetz
		\begin{equation}
			\vec{F} = m \cdot \vec{a}
		\end{equation}
		
		$\rho$ steht für die Dichte  $\frac{m}{V}$, $\vec{v}$ für die Geschwindigkeit, $t$ für die Zeit,
		$p$ für den Druck, $\mu$ ist die Viskosität und $\vec{f}$ steht für sämtliche weiteren
		Kräfte, u.a. Gravitation.\\
		$\nabla$ ist der Nabla-Operator, der den Gradienten einer Größe liefert, 
		$\nabla \cdot$	ist die Divergenz,	
		$\Delta$ ist der Laplace-Operator und ein Maß für die Abweichung einer Größe vom Durchschnitt.\\	
		
		Bei den beschriebenen physikalischen Größen handelt es sich um \emph{Kraft pro Volumen},
		"`Kraftdichten"' (engl. \emph{force density}).
		
		Die zweite Gleichung ist die Inkompressibilitätsbedingung.
		Die Forderung, dass die Divergenz der Geschwindigkeit überall null sein muss, heißt anschaulich, dass es keine
		Quellen oder Senken von Geschwindigkeitsvektoren geben darf. Damit bleibt die Dichte konstant, da immer genauso
		viel Materie einen Punkt verlässt wie "`herein kommt"'.\\

		Der Term 			
		$
				\frac{\partial \vec{v}}{\partial t} 
				+ 
				\left( \vec{v} \cdot \nabla	\right) \vec{v} 
		$
		beschreibt die \emph{Materielle Ableitung} und vereinfacht sich aufgrund der "`Lagrange'schen Natur"'
		der Partikeldomäne zu $\frac{D \vec{v}}{D t}$, da wir keinen Advektions-Term benötigen.\\

		
			
		Eine recht gute Einführung für den Laien in diese nicht ganz intuitiven Formeln, mit ein paar Beispielen
		und Herleitungen, liefert \cite{Steil2007}. Insbesondere wird hier die  
		"`Lagrange'sche"' und "`Eulersche"' Sicht, die über die Materielle Ableitung verknüpft sind,
		an einem anschaulichen Beispiel erläutert.\\
		
		Was wir für den weiteren Verlauf dieses Kapitels aus den Gleichungen \ref{equ:navStokes1} mitnehmen müssen,
		ist, dass wir für die mechanische Simulation von Fluiden Kraftdichten berechnen wollen,
		aus denen wir durch Division durch die Dichte die Beschleunigungen errechnen können,
		mithilfe derer dann die Integration geschieht, also die Aktualisierung von Positionen und Geschwindigkeiten.
			
		
			
				
		\subparagraph{Smoothed Particle Hydrodynamics}
		Das Verfahren wurde eigentlich von \cite{Gingold_Monaghan_1977} für Anwendung in der Astrophysik
		entwickelt, jedoch ist es auch für beliebige Fluidsimulationen einsetzbar.
		Es handelt sich um eine Interpolations-Methode: physikalische Größen sind nur stichprobenhaft
		in einem Raum enthalten, doch durch Interpolation von Nachbar-Samples lässt sich eine Größe $A$
		für jeden Punkt $\vec{r}$ im Raum bestimmen:
		
		\begin{equation} \label{equ:SPHbase}
			A_S(\vec{r}) = \sum_j m_j \frac{A_j}{\rho_j} W(\vec{r}-\vec{r}_j,h)
		\end{equation}
		
		Es wird über die $j$ Nachbar-Partikel in der Umgebung iteriert, und die Quantitäten aufsummiert,
		wobei sie mit dem Volumen ($\frac{m_i}{\rho_i}=V_i$) 
		sowie einem "`radial symmetric smoothing kernel"'  $W(\vec{r}_j,h)$ gewichtet werden.
		$h$ ist hier der \emph{Support Radius}. Der Kernel heißt "`normalisiert"' wenn sein Integral 1 ergibt:
		$\int W(\vec{r}) d \vec{r}=1$.\\
		
		
		Eine wichtige Eigenschaft ist, dass der Gradient oder der Laplacian
		einer Größe wie in Gleichung \ref{equ:SPHbase} zu berechnen ist, nur dass man den Gradienten bzw. den Laplacian
		des Smoothing Kernels verwendet:
		\begin{subequations}\label{equ:navStokes1}
			\begin{align}
			\nabla A_S(\vec{r}) &= \sum_j m_j \frac{A_j}{\rho_j} \nabla W(\vec{r}-\vec{r}_j,h) \\
			\Delta A_S(\vec{r}) &= \sum_j m_j \frac{A_j}{\rho_j} \Delta W(\vec{r}-\vec{r}_j,h)
			\end{align}	
		\end{subequations}
		
		Die Dichte an einem Punkt kann ebenfalls mit Gleichung \ref{equ:SPHbase} berechnet werden, wobei sich 
		gerade diese heraus kürzt:
		
		\begin{equation} \label{equ:SPHbase}
			\rho (\vec{r}) = \sum_j m_j  W(\vec{r}-\vec{r}_j,h)
		\end{equation}
		
		Je nach physikalischer Größe bieten sich unterschiedliche Sommting Kernels an, die besondere
		Eigenschaften haben.		
		
		%Es sei noch ein Wort über die Dichte verloren: Obwohl wir eigentlich inkompressible Flüssigkeiten
		%simulieren wollen, sorgen verschiedene Umstände dafür, dass die Dichte dennoch variiert: numerische Ungenauigkeit,
		%Dämpfung für numerische Stabilitä


		
		%ursprünglich aus astronomie blubb blubb 
		%\todo{überlegen, ob ich aus Interesse nicht noch weiter in die Richtung recherchieren sollte, da ich nach meiner 
		%Implementierung erst so richtig beeindruckt von dem Verfahren war (ich habe im Internet noch keine Fluid-Demo 
		%gefunden, die ebenfalls SPH implementiert; ok., ich hab auch nicht gesucht ;) ), und gerne mehr über die 
		%Hintergründe verstehen würde... problem, wie immer: Zeitdruck ;( }		
		
	\paragraph{Verwandte Arbeiten}
	\label{sec:relatedWork}

	Jos Stam \cite{Stam99stablefluids,Stam03realtimefluid} hat auf dem Gebiet der interaktiven Fluid-Simulation 
	Poinier-Arbeit geleistet 
	
	Referenzen auf Müller03, Thomas Steil, Goswami, GPU gems, Aufzeigen, was ich von wem übernommen habe, was ich selbst  
	modifiziert habe aufgrund von etwaigen Fehlern in den Papers odel weil OpenCL es schlciht nicht zulässt;
	

	\paragraph{Algorithmen}
	Verwaltung der Beschleunigungsstruktur ist der Löwenanteil, nicht die physiksimulation, die eher ein Dreizeiler 
	ist;
		\subparagraph{Work Efficient Parallel Prefix Sum}
		
		\subparagraph{Parallel Radix Sort und Stream Compaction}
		
	\paragraph{Ablauf}
		\label{sec:fluidSim:ablauf}
		initialisierung, und beschreibung der einzelnen phasen...

	\paragraph{Hardwarespezifische Optimierungen}
	\label{sec:hardwareOptimizations}
	
	
		\subparagraph{Bugs}
		\label{sec:oclBugs}
		Während der Implementierung sind mir einige Dinge aufgefallen, die ich auf Bugs im OpenCL-Treiber
		oder zumindest auf Beschränkungen der spezifischen OpenCL-Implementation zurück führe.
		Ich will nciht ausschließen, dass es sich vielleicht um eigene Programmierfahler handelt, jedoch habe ich
		viel herum probiert und meinen Code inspiziert, und mit bestimmten Code-Konstrukten, die eigentlich
		rein semantisch äquivalent sein müssten zu den Konstrukten, mit denen es Abstürze und/oder einen sonderbaren
		Simulationsverlauf gibt, funktioniert es dann.
		Teilweise ergaben sich mit der GTX280 Bugs, die auf der Geforce GT435M nicht auftraten;
		Dies kann z.T. an Architektur-spezifischen Unterschieden in den Treibern liegen, am verwendeten
		CUDA-Toolkit oder an dem signifikanten Unterschied bei der Anzahl an Compute Units (30 vs 2), die die
		GTX 280 potentiell anfälliger für Synchronisationsprobleme machen könnte.
		
		\begin{enumerate}
		
			\item Absturz bei globalen Speicherzugriffen (Lesen wie Schreiben) innerhalb von Schleifen, 
			wo sich die Schleifen-Lauf-Variablen zwischen den einzelnen work items einer
			Work Group unterscheiden:
			\begin{lstlisting}[language=OpenCL]
for(uint simGroupRunner=0; simGroupRunner < numSimWorkGroupsOfThisCell; simGroupRunner++ )
{
	//numSimWorkGroupsOfThisCell is different between work items here!
	/*global mem acces --> crash!*/
			\end{lstlisting}
			
			Work-Around: Feste Schleifen-Länge, Schleifen-Abbruch per \lstinline|break|, wenn eigentliche
			Schleifenbedingung nicht mehr gilt:
			\begin{lstlisting}[language=OpenCL]
for(uint simGroupRunner=0; simGroupRunner <  NUM_MAX_ALLOWED_UNIGRID_CELL_SPLIT_FACTOR; simGroupRunner++ )
{
	if( simGroupRunner >= numSimWorkGroupsOfThisCell )
    { break; }
    /*global mem acces --> works!*/
			\end{lstlisting}
			
		\item Bei wiederholtem lesenden Zugriff auf die selbe Stelle im 
		globalen Speicher innerhalb eines Schleifenrumpfes (selbst wenn Werte in eine 
		\lstinline|__private| -Variable zwischengespeichert und diese im Anschluss durchgehend genutzt werden),
		verhält sich die Simulation so, als ob bestimmte (andere als die erste)
		Operationen gar nicht ausgeführt worden wären; Es bestehen keinerlei Datenabhängikeiten zwischen
		den Operationen dieses Schleifenrumpfes;
		dieses Problem tritt nur mit der GTX280 auf, nicht mit der GT435M (GTX 570 konnte ich noch nicht testen)
		\begin{lstlisting}[language=OpenCL]
for(  uint interactingLocalIndex=0; 
           interactingLocalIndex < numNeighbourParticlesToInteractWith;
           interactingLocalIndex++ )
{  
	/*calc pressure force reading density twice and position once 
	from global mem resp. from just-refreshed private variable*/ 
	
	//barrier(CLK_LOCAL_MEM_FENCE); //<-- on GTX 280, this barrier must be commented in; 
	//it doesnt matter if its GLOBAL oder LOCAL memory fence, this is weird, as there is no local memory accces at all
	//with this kernel setup; 
	//On GT 435M, it works fine without this barrier...
	
	/*	calc viscosity force reading density once and density once from global mem 
		resp. from same private variable as for pressure calc
		<-- simulation acts like if there were no viscosity calculation at all without barrier!*/
}
		\end{lstlisting}
		
		\item 
		\label{enum:oclSyncBug}		
		Mit steigener Anzahl an Compute-Units und Partikeln wird die Simulation unter folgenden Umständen immer instabiler:
		Verteile ich den Scan, der für die Stream Compaction nötig ist, auf mehre Compute Units statt nur einer,
		so dass im Compaction Kernel pro Work Group noch ein kleiner finaler Scan gemacht werden muss,
		der die Teil-Summen der verschiedenen Scan-Work Groups aufsummiert, poppen Partikel
		in Zellen oder ganzen Zell-Blöcken weg, häufig friert die Anwendung ein; 
		Mit nur einer Compute Unit, die diesen Kernel abarbeitet, also nur einer Work group, ist die Simulation stabil.
		
		Von den bisher erwähnten Bugs halte ich es hier für am wahrscheinlichsten, dass es sich um einen Programmierfehler 
		meinerseits handelt, auch wenn ich bereits viel über diesen Kernel gegrübelt habe.
		
		Letztendlich ist der Kernel, der den Scan zu Stream Compaction macht, recht klein im Vergleich zu den
		Radix-Sort- und SPH-Kernels; Daher ist es für die Performance nicht so tragisch, wenn hier nur eine Work 
		Group aktiv ist; ärgerlich ist es trotzdem.
		\end{enumerate}

\clearpage
