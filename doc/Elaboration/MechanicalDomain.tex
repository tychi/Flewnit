

\label{sec:mechanicalDomain}
blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext 
	
\subsubsection{Fluidsimulation}
blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext 		
blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext 
blindtext blindtext blindtext blindtext blindtext 
		
	\paragraph{Grundlagen}
	blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext 
	
		\subparagraph{Die Navier-Stokes-Gleichungen}
		Herleitung, Erläuterung
		blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext 
			
		\subparagraph{Grid-basierte vs. Partikelbasierte Simulation}
		\label{par:GridVsParticle}
		blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext blindtext 
		blindtext blindtext blindtext 
			
		\subparagraph{Exkurs:Die zwei Sichtweisen: Lagrange vs. Euler}
		\label{par:lagrangeEuler}
		blindtext blindtext blindtext blindtext blindtext blindtext 
				
		\subparagraph{Smoothed Particle Hydrodynamics}
		ursprünglich aus astronomie blubb blubb 
		\todo{überlegen, ob ich aus Interesse nicht noch weiter in die Richtung recherchieren sollte, da ich nach meiner 
		Implementierung erst so richtig beeindruckt von dem Verfahren war (ich habe im Internet noch keine Fluid-Demo 
		gefunden, die ebenfalls SPH implementiert; ok., ich hab auch nicht gesucht ;) ), und gerne mehr über die 
		Hintergründe verstehen würde... problem, wie immer: Zeitdruck ;( }		
		
	\paragraph{Verwandte Arbeiten}
	\label{sec:relatedWork}
	Referenzen auf Müller03, Thomas Steil, Goswami, GPU gems, Aufzeigen, was ich von wem übernommen habe, was ich selbst  
	modifiziert habe aufgrund von etwaigen Fehlern in den Papers odel weil OpenCL es schlciht nicht zulässt;
	
	\paragraph{Umfang}
		Abgrenzungf zwischen bisher funktionierenden Features, bisher programmierten, aber nicht integrierten und 
		ungetesten Features und TODOs für die zukunft

	\paragraph{Algorithmen}
	Verwaltung der Beschleunigungsstruktur ist der Löwenanteil, nicht die physiksimulation, die eher ein Dreizeiler 
	ist;
		\subparagraph{Work Efficient Parallel Prefix Sum}
		
		\subparagraph{Parallel Radix Sort und Stream Compaction}
		
	\paragraph{Ablauf}
		\label{sec:fluidSim:ablauf}
		initialisierung, und beschreibung der einzelnen phasen...

	\paragraph{Hardwarespezifische Optimierungen}
	\label{sec:hardwareOptimizations}
	
		\subparagraph{Bugs}
		\label{sec:oclBugs}
		Während der Implementierung sind mir einige Dinge aufgefallen, die ich auf Bugs im OpenCL-Treiber
		oder zumindest auf Beschränkungen der spezifischen OpenCL-Implementation zurück führe.
		Ich will nciht ausschließen, dass es sich vielleicht um eigene Programmierfahler handelt, jedoch habe ich
		viel herum probiert und meinen Code inspiziert, und mit bestimmten Code-Konstrukten, die eigentlich
		rein semantisch äquivalent sein müssten zu den Konstrukten, mit denen es Abstürze und/oder einen sonderbaren
		Simulationsverlauf gibt, funktioniert es dann.
		Teilweise ergaben sich mit der GTX280 Bugs, die auf der Geforce GT435M nicht auftraten;
		Dies kann z.T. an Architektur-spezifischen Unterschieden in den Treibern liegen, am verwendeten
		CUDA-Toolkit oder an dem signifikanten Unterschied bei der Anzahl an Compute Units (30 vs 2), die die
		GTX 280 potentiell anfälliger für Synchronisationsprobleme machen könnte.
		
		\begin{enumerate}
		
			\item Absturz bei globalen Speicherzugriffen (Lesen wie Schreiben) innerhalb von Schleifen, 
			wo sich die Schleifen-Lauf-Variablen zwischen den einzelnen work items einer
			Work Group unterscheiden:
			\begin{lstlisting}[language=OpenCL]
for(uint simGroupRunner=0; simGroupRunner < numSimWorkGroupsOfThisCell; simGroupRunner++ )
{
	//numSimWorkGroupsOfThisCell is different between work items here!
	/*global mem acces --> crash!*/
			\end{lstlisting}
			
			Work-Around: Feste Schleifen-Länge, Schleifen-Abbruch per \lstinline|break|, wenn eigentliche
			Schleifenbedingung nicht mehr gilt:
			\begin{lstlisting}[language=OpenCL]
for(uint simGroupRunner=0; simGroupRunner <  NUM_MAX_ALLOWED_UNIGRID_CELL_SPLIT_FACTOR; simGroupRunner++ )
{
	if( simGroupRunner >= numSimWorkGroupsOfThisCell )
    { break; }
    /*global mem acces --> works!*/
			\end{lstlisting}
			
		\item Bei wiederholtem lesenden Zugriff auf die selbe Stelle im 
		globalen Speicher innerhalb eines Schleifenrumpfes (selbst wenn Werte in eine 
		\lstinline|__private| -Variable zwischengespeichert und diese im Anschluss durchgehend genutzt werden),
		verhält sich die Simulation so, als ob bestimmte (andere als die erste)
		Operationen gar nicht ausgeführt worden wären; Es bestehen keinerlei Datenabhängikeiten zwischen
		den Operationen dieses Schleifenrumpfes;
		dieses Problem tritt nur mit der GTX280 auf, nicht mit der GT435M (GTX 570 konnte ich noch nicht testen)
		\begin{lstlisting}[language=OpenCL]
for(  uint interactingLocalIndex=0; 
           interactingLocalIndex < numNeighbourParticlesToInteractWith;
           interactingLocalIndex++ )
{  
	/*calc pressure force reading density twice and position once 
	from global mem resp. from just-refreshed private variable*/ 
	
	//barrier(CLK_LOCAL_MEM_FENCE); //<-- on GTX 280, this barrier must be commented in; 
	//it doesnt matter if its GLOBAL oder LOCAL memory fence, this is weird, as there is no local memory accces at all
	//with this kernel setup; 
	//On GT 435M, it works fine without this barrier...
	
	/*	calc viscosity force reading density once and density once from global mem 
		resp. from same private variable as for pressure calc
		<-- simulation acts like if there were no viscosity calculation at all without barrier!*/
}
		\end{lstlisting}
		
		\item 
		\label{enum:oclSyncBug}		
		Mit steigener Anzahl an Compute-Units und Partikeln wird die Simulation unter folgenden Umständen immer instabiler:
		Verteile ich den Scan, der für die Stream Compaction nötig ist, auf mehre Compute Units statt nur einer,
		so dass im Compaction Kernel pro Work Group noch ein kleiner finaler Scan gemacht werden muss,
		der die Teil-Summen der verschiedenen Scan-Work Groups aufsummiert, poppen Partikel
		in Zellen oder ganzen Zell-Blöcken weg, häufig friert die Anwendung ein; 
		Mit nur einer Compute Unit, die diesen Kernel abarbeitet, also nur einer Work group, ist die Simulation stabil.
		
		Von den bisher erwähnten Bugs halte ich es hier für am wahrscheinlichsten, dass es sich um einen Programmierfehler 
		meinerseits handelt, auch wenn ich bereits viel über diesen Kernel gegrübelt habe.
		
		Letztendlich ist der Kernel, der den Scan zu Stream Compaction macht, recht klein im Vergleich zu den
		Radix-Sort- und SPH-Kernels; Daher ist es für die Performance nicht so tragisch, wenn hier nur eine Work 
		Group aktiv ist; ärgerlich ist es trotzdem.
		\end{enumerate}

\clearpage
